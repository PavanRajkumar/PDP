# -*- coding: utf-8 -*-
"""Parkinsons_randomForest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fQjyXjv7UkzNZafaswt71TuDIBahF8nf
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My\ Drive/ML/Parkinsons

import numpy as np
import matplotlib as mlt
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold

from sklearn.metrics import accuracy_score,f1_score,recall_score, precision_score, confusion_matrix,matthews_corrcoef

data = pd.read_csv('train_data.txt', sep=",", header=None)
data.columns = ["Subject id", "Jitter (local)" ,"Jitter (local, absolute)" ,"Jitter (rap)" ,"Jitter (ppq5)", "Jitter (ddp)", "Shimmer (local)", "Shimmer (local, dB)" ,"Shimmer (apq3)", "Shimmer (apq5)", "Shimmer (apq11)", "Shimmer (dda)", "AC" ,"NTH" ,"HTN", "Median pitch", "Mean pitch", "Standard deviation", "Minimum pitch", "Maximum pitch", "Number of pulses", "Number of periods", "Mean period", "Standard deviation of period", "Fraction of locally unvoiced frames", "Number of voice breaks", "Degree of voice breaks", "UPDRS", "class information"]
data.shape

y_updrs = data.UPDRS # for regression
y_class_info = data["class information"]#for classification
data_x = data.drop(labels=['UPDRS', "Subject id", "class information"], axis=1, inplace = False)
print(data_x.shape, y_updrs.shape, y_class_info.shape)

total_folds = 10
kfold = KFold(n_splits= total_folds, shuffle=True, random_state=40)

acc_per_fold = []
fold=0

for train_index, test_index in kfold.split(data_x, y_class_info):
    fold+=1
    X_train, X_test, y_train, y_test =  data_x.iloc[train_index], data_x.iloc[test_index], y_class_info.iloc[train_index], y_class_info.iloc[test_index]

    clf = RandomForestClassifier(min_samples_split=2, 
                             bootstrap= True,
                             oob_score = True,  
                             random_state=0,
                             min_samples_leaf= 1,
                             n_estimators = 400,
                             max_depth= 70)

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)
    
    print("\nFold : ",fold,"\n")
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

    print("Average Accuracy : " ,(tp+tn)/(tp+tn+fp+fn))
    print("Sensitivity: ",(tp)/(tp+fn))
    print("Specificity: ",(tn)/(tn+fp))
    print("MCC: ",matthews_corrcoef(y_test, y_pred))
    print(clf.score(X_test, y_test))

    acc_per_fold.append(clf.score(X_test, y_test))

for i in range(total_folds):
    print("Fold ", i, "accuracy", acc_per_fold[i])

print("\nMean Accuracy = ",np.mean(acc_per_fold),'(+/-',np.std(acc_per_fold),')');

X_train, X_test, y_train, y_test = train_test_split(data_x, y_class_info, test_size = 0.20, random_state = 42)
clf_final = RandomForestClassifier(min_samples_split=2, 
                             bootstrap= True,
                             oob_score = True,  
                             random_state=0,
                             min_samples_leaf= 1,
                             n_estimators = 400,
                             max_depth= 70)

clf_final.fit(X_train, y_train)

y_pred = clf_final.predict(X_test)
y_score = clf_final.predict_proba(X_test)[:,1]

print(clf_final.score(X_test, y_test))
print("Accuracy Score = ",accuracy_score(y_test, y_pred, normalize=True))

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

print("Average Accuracy : " ,(tp+tn)/(tp+tn+fp+fn))
print("Sensitivity: ",(tp)/(tp+fn))
print("Specificity: ",(tn)/(tn+fp))
print("MCC: ",matthews_corrcoef(y_test, y_pred))
print("F-Measure : ", f1_score(y_test, y_pred))
print("Confusion Matrix = ",confusion_matrix(y_test, y_pred, labels=None, sample_weight=None, normalize=None))

import seaborn as sn

array = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(array, index = [i for i in ["non - PD", "PD"]],
                  columns = [i for i in ["non-PD","PD"]])
accuracy_thr = accuracy_score(y_test, y_pred)

plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)
sn.heatmap(df_cm,cmap="YlGnBu",annot=True,annot_kws={"fontsize":48})
plt.savefig("RF_Conf_orig_matrix.png", bbox_inches = "tight")

X_test.iloc[1].reshape(1,-1).shape

interpretation = pd.DataFrame({'Variable':data_x.columns,   'Importance':clf_final.feature_importances_}).sort_values('Importance', ascending=False)
interpretation

sum(interpretation['Importance'].to_list())

features = data_x.columns.tolist()
features

importance = clf_final.feature_importances_

for i,v in zip(features, importance):
	print('%s, Score: %.5f' % (i,v))

print(len(y_test), len(y_pred), len(y_score))

import pandas as pd
from numpy import sqrt
from numpy import argmax
from sklearn.metrics import auc, accuracy_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve, confusion_matrix

fpr, tpr, thresholds = roc_curve(y_test, y_score)
roc_auc = auc(fpr, tpr)
gmeans = sqrt(tpr * (1-fpr))
arg_g = argmax(gmeans)
print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[arg_g], gmeans[arg_g]))
best_threshold = thresholds[arg_g] 
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
acc = accuracy_score(y_test, y_pred)
# tpr = tp/(tp+fn)
# fpr = fp/(fp+tn)
# tnr = tn/(fp+tn)
# fnr = fn/(tp+fn)

print("accuracy = ", acc, "\nroc_auc = ", roc_auc)


i=0
roc_table = pd.DataFrame(columns=["Threshold", "TPR (Sensitivity)",
                                  "FPR (Fall-out)", "Specificity",
                                  " (LR+)" ,"Youden index","Sensitivity + Specificity",
                                  "G-mean"], index=[_ for _ in range(len(thresholds))])

for fp_rate, tp_rate, thresh in zip(fpr, tpr, thresholds):
    spec = 1-fp_rate
    lrplus = tp_rate/fp_rate
    y_ind = tp_rate - fp_rate
    sen_sp = tp_rate + spec
    g_mean =  (tp_rate*spec)**(1/2)
    roc_table.iloc[i] = [thresh, tp_rate, fp_rate, spec, lrplus, y_ind, sen_sp, g_mean ]
    i+=1

# Commented out IPython magic to ensure Python compatibility.
# %cd ../..

pd.set_option("display.precision", 4)
roc_table.to_csv("RF_ROC_Table.csv")
roc_table

plt.subplots(1, figsize=(7,7))
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, marker = "." ,label = 'AUC = %0.2f' % roc_auc)
#plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'--', label = "No Skill")

plt.scatter(fpr[arg_g], tpr[arg_g], marker='o', color='black', label='Best')
# plt.xlim([0, 1])
# plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(["ROC", "No Skill"], loc ="lower right")

plt.savefig("RF_ROC.jpg", bbox_inches="tight")

plt.show()

precision, recall, thresholds = precision_recall_curve(y_test, y_score)

fscore_ = (2 * precision * recall) / (precision + recall)
# locate the index of the largest f score
ix = argmax(fscore_)
print('Best Threshold=%f, F-Score=%.3f' % (thresholds[ix], fscore_[ix]))

pr_table = pd.DataFrame(columns=["Threshold", "Precision",
                                  "Recall", "F-Measure"], 
                                index=[_ for _ in range(len(thresholds))])

i=0
for pre, rec, thresh in zip(precision, recall, thresholds):
    fscore = (2 * pre * rec) / (pre + rec)
    pr_table.iloc[i] = [thresh, pre, rec, fscore]
    i+=1

pr_table.to_csv("RF_PR_Table.csv")
pr_table

plt.subplots(1, figsize=(7,7))
plt.title('Precision-Recall Curve')
plt.plot(recall, precision)
plt.plot([0, 1], [0.725, 0.725], linestyle='--')
plt.legend(loc = 'lower right')
# plt.xlim([0, 1])
# plt.ylim([0, 1])
plt.scatter(recall[ix], precision[ix], marker='o', color='black', label='Best')
# axis labels
plt.ylabel('Precision')
plt.xlabel('Recall')
plt.legend(["PR","No Skill"], loc ="upper right")

plt.savefig("RF_PR.jpg", bbox_inches="tight")
plt.show()

y_pred_thr = [ 1 if _ >= best_threshold else 0 for _ in clf_final.predict_proba(X_test)[:,1]]
print(best_threshold)
accuracy_thr = accuracy_score(y_test, y_pred_thr)
print("New accuracy ",accuracy_thr)
tn_thr, fp_thr, fn_thr, tp_thr = confusion_matrix(y_test, y_pred_thr).ravel()

print("Average Accuracy : " ,(tp_thr+tn_thr)/(tp_thr+tn_thr+fp_thr+fn_thr))
print("Sensitivity: ",(tp_thr)/(tp_thr+fn_thr))
print("Specificity: ",(tn_thr)/(tn_thr+fp_thr))
print("MCC: ",matthews_corrcoef(y_test, y_pred_thr))

import seaborn as sn

array = confusion_matrix(y_test, y_pred_thr)
df_cm = pd.DataFrame(array, index = [i for i in ["non - PD", "PD"]],
                  columns = [i for i in ["non-PD","PD"]])
accuracy_thr = accuracy_score(y_test, y_pred_thr)

plt.figure(figsize = (10,7))
sn.set(font_scale=1.4)
sn.heatmap(df_cm,cmap="YlGnBu",annot=True,annot_kws={"fontsize":48})
plt.savefig("RF_Conf_matrix.png", bbox_inches = "tight")

# Calculate feature importances

indices = np.argsort(importance)[::-1]
indices = indices[:15]
# Rearrange feature names so they match the sorted feature importances
names = [features[i] for i in indices]

# Barplot: Add bars
plt.subplots(1, figsize=(14,7))
plt.bar(range(len(indices)), importance[indices])  #range(X_test.shape[1])
# Add feature names as x-axis labels
plt.xticks(range(len(indices)), names, rotation=60, fontsize = 10)   #range(X_test.shape[1]
# Create plot title
plt.title("Feature Importance")
# Show plot
plt.show()

